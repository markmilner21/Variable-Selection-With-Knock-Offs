---
title: "Model-X knockoffs"
author: "Mark Milner"
date: "2025-01-16"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This is a playground for the implementation of the Model-X knockoffs. Here, we assume the covariate distribution of X is known but do not know how the relationship between target and covariates can be modeled. By default, the knockoff filter creates model-X second-order Gaussian knockoffs (see the panning for gold section on approximate construction of Model-X knockoffs) 

## Data Setup 

```{r}
set.seed(21)
```

### Defining our problem parameters
```{r}
n = 1000 # Suppose that we have 1000 rows of data, each independent from the others
p = 1000 # Suppose that we have 1000 columns corresponding to 1000 covariates. Since we are implimenting Model-X knockoffs, we are not bound by any constraints concerning the relationship between n and p (namely n >= 2p)

relevant_count = 60 # Suppose that the total number of relevant variables is 60
amplitude = 4.5   # signal amplitude (for noise level = 1). This is implimented for noise reasons as real world data will have a degree of noise.
```

### Generating our covariate data
```{r}
mu = rep(0,p)
rho = 0.25 # correlation coefficient - used in constructing the covariance matrix to define the correlation structure between different variables of the multivariate normal distribution
Sigma = toeplitz(rho^(0:(p-1)))
X = matrix(rnorm(n*p),n) %*% chol(Sigma)

# generate n x p independent standard normal random variables (with mean 0 and variance 1) and reshaped into an n-row matrix. each row corresponds to an observartion each column a variable. 
# that weird symbol is matrix multiplication 
# his computes the Cholesky decomposition of the covariance matrix Sigma
# Final Step: The matrix X contains multivariate normal data where each row is a random sample from a multivariate normal distribution with mean 
#Œº=0 and covariance matrix Œ£. The matrix multiplication 
#matrix xchol(Sigma) transforms the independent normal random variables into correlated normal variables. 
# so now we have created our synthetic random variables
```

### Generate our target data
```{r}
# Here, we show how we generate the response from a linear mode
nonzero = sample(p, relevant_count)
# This function randomly selects k distinct integers from the set {1,2,‚Ä¶,ùëù} p is the total number of predictors (features). these will be the truly relevant covariates.
beta = amplitude * (1:p %in% nonzero) / sqrt(n)
# this vector determines the coefficients of the variables and so which predictors are relevant and they have been scaled according to amplitude and sqrt(n). all others are 0 which means not relevant.
y.sample = function(X) X %*% beta + rnorm(n)
# simulates the response y through a linear model. rnorm is the noise which is distributed under a standard normal 
# generates the values of y according to the true relevant variables which we have randomely chosen. In a real setting, we would not know how y was created but in this setting we are simply imagining. In this framework remeber we know the distribution of the covariates but nothing about the relationship between the covairates and target variable

y = y.sample(X)
# calls the function on the design matrix X. so we generate the data of y based on X. In reality we wouldn't know this relationship and so we wouldn't necessarily be able to do this.  
```

By default, the knockoff filter creates model-X second-order Gaussian knockoffs. This construction estimates from the data the mean $\mu$ and the covariance $\Sigma$ of the rows of $X$, instead of using the true parameters ($\mu, \Sigma$) from which the variables were sampled. X is typically sampled from an unknown distribution, so the parameters (ùúá,Œ£) must be estimated from the observed data. Estimating these parameters ensures the knockoff construction is feasible even when the true underlying generative model is inaccessible.

The knockoff package also includes other knockoff construction methods, all of which have names prefixed with`knockoff.create`. In the next snippet, we generate knockoffs using the true model parameters.

### Testing our variable selection method. 

#### Eperiment 1

Default: By default, we generate knockoffs as second-order Gaussian knockoffs
```{r}
library(knockoff)
result_default = knockoff.filter(X, y)
#  This function runs the Knockoffs procedure from start to finish, selecting variables relevant for predicting the outcome of interest.
#  knockoff.filter(X,y,knockoffs = create.second_order,statistic = stat.glmnet_coefdiff,fdr = 0.1,offset = 1)
#  method used to construct knockoffs for the X variables. It must be a function taking a n-by-p matrix as input and returning a n-by-p matrix of knockoff variables. By default, approximate model-X Gaussian knockoffs are used.
# statistics used to assess variable importance. By default, a lasso statistic with cross-validation is used.
# target false discovery rate (default: 0.1).
# either 0 or 1 (default: 1). This is the offset used to compute the rejection threshold on the statistics. The value 1 yields a slightly more conservative procedure ("knockoffs+") that controls the false discovery rate (FDR) according to the usual definition, while an offset of 0 controls a modified FDR.
```

By default, the knockoff filter uses a test statistic based on the lasso. Specifically, it uses the statistic `stat.glmnet_coefdiff`, which computes $$ W_j = |Z_j| - |\tilde{Z}_j| $$

where $Z_j$ and $\tilde{Z}_j$ are the lasso coefficient estimates for the jth variable and its knockoff, respectively. The value of the regularization parameter $\lambda$ is selected by cross-validation and computed with `glmnet`.

#### Eperiment 2 - knockoffs generated using the true parameters

Alternatively, we can generate knockoffs using the true model parameters.
```{r knock-gaussian}
gaussian_knockoffs = function(X) create.gaussian(X, mu, Sigma)
result_true_gaussian = knockoff.filter(X, y, knockoffs=gaussian_knockoffs)
```
#### Experiment 3 - random forest importance statistic and higher permitted false discovery rate

Several other built-in statistics are available, all of which have names prefixed with `stat`. For example, we can use statistics based on random forests. In addition to choosing different statistics, we can also vary the target FDR level (e.g. we now increase it to 0.2).

```{r knock-RF}
result_rf = knockoff.filter(X, y, knockoffs = gaussian_knockoffs, statistic = stat.random_forest, fdr=0.2)
```

#### Experiment 4 - User-defined variable importance statistics

In addition to using the predefined test statistics, it is also possible to use your own custom test statistics. To illustrate this functionality, we implement one of the simplest test statistics from the original knockoff filter paper, namely $$ W_j = \left|X_j^\top \cdot y\right| - \left|\tilde{X}_j^\top \cdot y\right|.$$
```{r custom-stat, warning=FALSE}
my_knockoff_stat = function(X, X_k, y) {
  abs(t(X) %*% y) - abs(t(X_k) %*% y)
}
result_user_defined_VI = knockoff.filter(X, y, knockoffs = gaussian_knockoffs, statistic = my_knockoff_stat)
```

#### Experiment 5 - User-defined knockoff generating functions

In addition to using the predefined procedures for construction knockoff variables, it is also possible to create your own knockoffs. To illustrate this functionality, we implement a simple wrapper for the construction of second-order Model-X knockoffs.

```{r custom-knock}
create_knockoffs = function(X) {
  create.second_order(X, shrink=T)
}
result_user_defined_kgf = knockoff.filter(X, y, knockoffs=create_knockoffs)
```

#### Experiment 6 - Approximate vs Full SDP knockoffs

The knockoff package supports two main styles of knockoff variables, *semidefinite programming* (SDP) knockoffs (the default) and *equi-correlated* knockoffs. Though more computationally expensive, the SDP knockoffs are statistically superior by having higher power (page 14).To create SDP knockoffs, this package relies on the R library [Rdsdp][Rdsdp] to efficiently solve the semidefinite program.

In high-dimensional settings, this program becomes computationally intractable.
A solution is then offered by approximate SDP (ASDP) knockoffs, which address this issue by solving a simpler relaxed problem based on a block-diagonal approximation of the covariance matrix. By default, the knockoff filter uses SDP knockoffs if $p<500$ and ASDP knockoffs otherwise.

In this example we generate second-order Gaussian knockoffs using the estimated model parameters and the full SDP construction. Then, we run the knockoff filter as usual.
```{r knock-second-order}
gaussian_knockoffs = function(X) create.second_order(X, method='sdp', shrink=T)
result_knockoff_construction1 = knockoff.filter(X, y, knockoffs = gaussian_knockoffs)
```

#### Experiment 7 - Equi-correlated knockoffs

Equicorrelated knockoffs offer a computationally cheaper alternative to SDP knockoffs, at the cost of lower statistical power.
In this example we generate second-order Gaussian knockoffs using the estimated model parameters and the equicorrelated construction. Then we run the knockoff filter.

```{r knock-equi}
gaussian_knockoffs = function(X) create.second_order(X, method='equi', shrink=T)
result_knockoff_construction2 = knockoff.filter(X, y, knockoffs = gaussian_knockoffs)
```


### Results evaluation 

```{r}
print('The following are the truely relevant variables. In this case, that would be those variables with non-zero coefficients in the model. Recall we have sample y according to beta')
print(nonzero)
```

```{r}
print('The following are the results considered relevant according to our knockoff-based variable selection process under default parameters')
print(result$selected)
```
We won't do this for all the examples but lets consider the important metrics of each experiment

### False Discovery proportion

Now, recall in all the examples, the FDR is set to default of 0.1 (besides the being explicity changed in experiment 3 to 0.2)

```{r}
fdp = function(selected) sum(beta[selected] == 0) / max(1, length(selected))
```

```{r}
fdp(result_default$selected)
fdp(result_true_gaussian$selected)
fdp(result_rf$selected)
fdp(result_user_defined_VI$selected)
fdp(result_user_defined_kgf$selected)
fdp(result_knockoff_construction1$selected)
fdp(result_knockoff_construction2$selected)
```
### Power

```{r}
power = function(selected) {
  # True Positives: selected variables with non-zero coefficients
  TP = sum(beta[selected] != 0)
  
  # Total Relevant Variables
  T = sum(beta != 0)
  
  # Compute Power
  TP / T
}
```

```{r}
power(result_default$selected)
power(result_true_gaussian$selected)
power(result_rf$selected)
power(result_user_defined_VI$selected)
power(result_user_defined_kgf$selected)
power(result_knockoff_construction1$selected)
power(result_knockoff_construction2$selected)

```


```{r}
fdp(result_default$selected)
power(result_default$selected)
```




### Visualisation

```{r}
# Create a data frame of results
results_aggregated = data.frame(
  Experiment = c("Default", "True Gaussian", "Random Forest", "User Defined VI", 
                 "User Defined KGF", "SDP Knockoffs", "Equicorrelated Knockoffs"),
  FDP = c(
    fdp(result_default$selected),
    fdp(result_true_gaussian$selected),
    fdp(result_rf$selected),
    fdp(result_user_defined_VI$selected),
    fdp(result_user_defined_kgf$selected),
    fdp(result_knockoff_construction1$selected),
    fdp(result_knockoff_construction2$selected)
  ),
  Power = c(
    power(result_default$selected),
    power(result_true_gaussian$selected),
    power(result_rf$selected),
    power(result_user_defined_VI$selected),
    power(result_user_defined_kgf$selected),
    power(result_knockoff_construction1$selected),
    power(result_knockoff_construction2$selected)
  )
)

```

#### FDR

```{r}
library(ggplot2)

# Bar plot for FDP with an additional horizontal line at FDR = 0.1
ggplot(results_aggregated, aes(x = Experiment, y = FDP, fill = Experiment)) +
  geom_bar(stat = "identity", color = "black") +
geom_hline(yintercept = 0.1, linetype = "dashed", color = "red", size = 1) + # Add horizontal line
  theme_minimal() +
  labs(title = "False Discovery Proportion (FDP)", 
       x = "Experiment", 
       y = "FDP") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set3") +
  scale_y_continuous(limits = c(0, 1))
```

#### Power

```{r}
# Bar plot for Power
ggplot(results_aggregated, aes(x = Experiment, y = Power, fill = Experiment)) +
  geom_bar(stat = "identity", color = "black") +
  theme_minimal() +
  labs(title = "Power", 
       x = "Experiment", 
       y = "Power") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set3") +
  scale_y_continuous(limits = c(0, 1))

```

#### Test Plots

```{r}
# Simulate data for multiple runs (example structure)
set.seed(123)
experiment_results <- data.frame(
  Experiment = rep(c("Default", "Gaussian", "Random Forest"), each = 100),
  FDP = c(
    rnorm(100, mean = 0.1, sd = 0.02),
    rnorm(100, mean = 0.12, sd = 0.03),
    rnorm(100, mean = 0.15, sd = 0.025)
  )
)

# Calculate mean and confidence intervals
library(dplyr)

results_summary <- experiment_results %>%
  group_by(Experiment) %>%
  summarise(
    Mean_FDP = mean(FDP),
    Lower_CI = Mean_FDP - qt(0.975, df = n() - 1) * sd(FDP) / sqrt(n()),
    Upper_CI = Mean_FDP + qt(0.975, df = n() - 1) * sd(FDP) / sqrt(n())
  )

# Plot with shaded confidence intervals
library(ggplot2)

ggplot(results_summary, aes(x = Experiment, y = Mean_FDP, fill = Experiment)) +
  geom_bar(stat = "identity", color = "black") +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI), width = 0.2, color = "black") +
  geom_hline(yintercept = 0.1, linetype = "dashed", color = "red", size = 1) + # FDR line
  geom_ribbon(aes(ymin = Lower_CI, ymax = Upper_CI, fill = Experiment), alpha = 0.2) + # Shaded CI
  theme_minimal() +
  labs(
    title = "False Discovery Proportion (FDP) with Confidence Intervals",
    x = "Experiment",
    y = "Mean FDP"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  scale_fill_brewer(palette = "Set3") +
  scale_y_continuous(limits = c(0, 1))

```

```{r}
# Define expected FDR levels
Expected_FDR = seq(0.05, 0.25, by = 0.025)
# Initialize a vector to store observed FDP
Observed_FDP = numeric(length(Expected_FDR))
# Loop through each FDR level and compute FDP
for (i in seq_along(Expected_FDR)) {
  current_fdr = Expected_FDR[i]
  # Run the knockoff filter with the current FDR level
  current_result = knockoff.filter(X, y, fdr = current_fdr)
  # Compute the FDP for the selected variables
  current_fdp = fdp(current_result$selected)
  # Store the FDP
  Observed_FDP[i] = current_fdp
}

```




```{r}
# Simulate expected and observed FDR-FDP pairs
simulated_results <- data.frame(
  Expected_FDR = seq(0.05, 0.25, by = 0.025),
  Observed_FDP
)

# Plotting FDR vs. FDP with y=x line
library(ggplot2)

ggplot(simulated_results, aes(x = Expected_FDR, y = Observed_FDP)) +
  geom_point(size = 4, color = "blue") + # Points for FDP vs. FDR
  geom_line(color = "blue", linetype = "solid") + # Line connecting points
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", size = 1) + # y=x line
  theme_minimal() +
  labs(
    title = "Observed FDP vs. Expected FDR",
    x = "Expected FDR (q)",
    y = "Observed FDP"
  ) +
  theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14))

```

# implimenting repeated experiments for each q (the above only does 1 test per level of Q)
```{r}
Expected_FDR = seq(0.05, 0.25, by = 0.025)
# Initialize a vector to store observed FDP
Observed_FDP = numeric(length(Expected_FDR))
# Loop through each FDR level and compute FDP
for (i in seq_along(Expected_FDR)) {
  current_tally = 0
  for (j in (1:5)){
    current_fdr = Expected_FDR[i]
    # Run the knockoff filter with the current FDR level
    current_result = knockoff.filter(X, y, fdr = current_fdr)
    # Compute the FDP for the selected variables
    current_fdp = fdp(current_result$selected)
    current_tally = current_tally + current_fdp
  }
  Observed_average_FDP[i] = current_fdp / 5
}

```

```{r}
# Simulate expected and observed FDR-FDP pairs
simulated_results <- data.frame(
  Expected_FDR = seq(0.05, 0.25, by = 0.025),
  Observed_average_FDP
)

# Plotting FDR vs. FDP with y=x line
library(ggplot2)

ggplot(simulated_results, aes(x = Expected_FDR, y = Observed_FDP)) +
  geom_point(size = 4, color = "blue") + # Points for FDP vs. FDR
  geom_line(color = "blue", linetype = "solid") + # Line connecting points
  geom_abline(intercept = 0, slope = 1, linetype = "dashed", color = "red", size = 1) + # y=x line
  theme_minimal() +
  labs(
    title = "Observed FDP vs. Expected FDR",
    x = "Expected FDR (q)",
    y = "Observed FDP"
  ) +
  theme(axis.text = element_text(size = 12), axis.title = element_text(size = 14))

```

```{r}
Observed_average_FDP
```

```{r}

```

